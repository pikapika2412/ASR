import requests
from bs4 import BeautifulSoup
import sqlite3
import json
import time
import re
import random
from urllib.parse import urljoin

class DuyTanLibraryScraper:
    def __init__(self):
        self.base_url = "https://elib.duytan.edu.vn"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        
    def extract_book_info(self, book_url):
        """Extract detailed information from a book page"""
        try:
            print(f"ƒêang tr√≠ch xu·∫•t th√¥ng tin t·ª´: {book_url}")
            response = self.session.get(book_url)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser')
            
            book_info = {}
              # Extract book title - Simple method using HTML structure
            title_element = soup.find('li', class_='title')
            if title_element:
                book_info['title'] = title_element.get_text().strip()
            
            # Extract all text content for other metadata parsing
            page_text = soup.get_text()
            
            # Extract book cover image
            img_element = soup.find('img', src=re.compile(r'sach_anh'))
            if img_element:
                book_info['cover_image'] = urljoin(self.base_url, img_element['src'])
            
            # Extract author - T√°c gi·∫£
            author_match = re.search(r'T√°c gi·∫£:\s*([^\n\r]+)', page_text)
            if author_match:
                book_info['author'] = author_match.group(1).strip()
            
            # Extract publisher - Nh√† xu·∫•t b·∫£n
            publisher_match = re.search(r'Nh√† xu·∫•t b·∫£n:\s*([^\n\r]+)', page_text)
            if publisher_match:
                book_info['publisher'] = publisher_match.group(1).strip()
            
            # Extract publication year - NƒÉm xu·∫•t b·∫£n
            year_match = re.search(r'NƒÉm xu·∫•t b·∫£n:\s*(\d{4})', page_text)
            if year_match:
                book_info['publication_year'] = int(year_match.group(1))
            
            # Extract number of pages - S·ªë trang
            pages_match = re.search(r'S·ªë trang:\s*([^\n\r]+)', page_text)
            if pages_match:
                book_info['pages'] = pages_match.group(1).strip()
            
            # Extract dimensions - K√≠ch th∆∞·ªõc
            dimensions_match = re.search(r'K√≠ch th∆∞·ªõc:\s*([^\n\r]+)', page_text)
            if dimensions_match:
                book_info['dimensions'] = dimensions_match.group(1).strip()
            
            # Extract registration number - S·ªë ƒëƒÉng k√Ω c√° bi·ªát
            reg_number_match = re.search(r'S·ªë ƒëƒÉng k√Ω c√° bi·ªát:\s*([^\n\r]+)', page_text)
            if reg_number_match:
                book_info['registration_number'] = reg_number_match.group(1).strip()
            
            # Extract ISBN
            isbn_match = re.search(r'ISBN:\s*([^\n\r]+)', page_text)
            if isbn_match:
                book_info['isbn'] = isbn_match.group(1).strip()
            
            # Extract Dewey code - M√£ Dewey
            dewey_match = re.search(r'M√£ Dewey:\s*([^\n\r]+)', page_text)
            if dewey_match:
                book_info['dewey_code'] = dewey_match.group(1).strip()
            
            # Extract price - ƒê∆°n gi√°
            price_match = re.search(r'ƒê∆°n gi√°:\s*([^\n\r]+)', page_text)
            if price_match:
                book_info['price'] = price_match.group(1).strip()
            
            # Extract storage location - V·ªã tr√≠ l∆∞u tr·ªØ
            location_match = re.search(r'V·ªã tr√≠ l∆∞u tr·ªØ:\s*([^\n\r]+)', page_text)
            if location_match:
                book_info['storage_location'] = location_match.group(1).strip()
            
            # Extract language - Ng√¥n ng·ªØ
            language_match = re.search(r'Ng√¥n ng·ªØ:\s*([^\n\r]+)', page_text)
            if language_match:
                book_info['language'] = language_match.group(1).strip()
            
            # Extract document type - Lo·∫°i t√†i li·ªáu
            doc_type_match = re.search(r'Lo·∫°i t√†i li·ªáu:\s*([^\n\r]+)', page_text)
            if doc_type_match:
                book_info['document_type'] = doc_type_match.group(1).strip()
            
            # Extract availability - ƒêang r·ªói/ T·ªïng s√°ch
            availability_match = re.search(r'ƒêang r·ªói/ T·ªïng s√°ch:\s*([^\n\r]+)', page_text)
            if availability_match:
                book_info['availability'] = availability_match.group(1).strip()
            
            # Extract keywords - T·ª´ kh√≥a
            keywords_match = re.search(r'T·ª´ kh√≥a:\s*([^\n\r]+)', page_text)
            if keywords_match:
                book_info['keywords'] = keywords_match.group(1).strip()
            
            # Extract subject - Ch·ªß ƒë·ªÅ
            subject_match = re.search(r'Ch·ªß ƒë·ªÅ:\s*([^\n\r]+)', page_text)
            if subject_match:
                book_info['subject'] = subject_match.group(1).strip()
            
            # Extract department - Chuy√™n ng√†nh
            department_match = re.search(r'Chuy√™n ng√†nh:\s*([^\n\r]+)', page_text)
            if department_match:
                book_info['department'] = department_match.group(1).strip()
              # Extract summary - T√≥m t·∫Øt
            summary_match = re.search(r'T√≥m t·∫Øt:\s*(.*?)(?=\n\s*\n|\n\s*[A-Z]|$)', page_text, re.DOTALL)
            if summary_match:
                summary = summary_match.group(1).strip()
                summary = re.sub(r'\s+', ' ', summary)
                summary = summary.split('B·∫°n ph·∫£i ƒëƒÉng nh·∫≠p')[0].strip()
                if summary:
                    book_info['summary'] = summary
            
            # Add URL for reference
            book_info['url'] = book_url
            
            print(f"‚úì ƒê√£ tr√≠ch xu·∫•t th√†nh c√¥ng: {book_info.get('title', 'Kh√¥ng c√≥ ti√™u ƒë·ªÅ')}")
            return book_info
            
        except Exception as e:
            print(f"‚úó L·ªói khi tr√≠ch xu·∫•t th√¥ng tin t·ª´ {book_url}: {str(e)}")
            return {}

    def get_random_books(self, num_books=36):
        """Get random book URLs from random pages in the library"""
        book_urls = []
        attempts = 0
        max_attempts = num_books * 3  # Try 3 times more than needed
        
        print(f"üîç B·∫Øt ƒë·∫ßu thu th·∫≠p {num_books} quy·ªÉn s√°ch ng·∫´u nhi√™n...")
        
        while len(book_urls) < num_books and attempts < max_attempts:
            # Generate random page number (limit to first 200 pages for efficiency)
            random_page = random.randint(1, 200)
            
            try:
                print(f"üìñ ƒêang l·∫•y trang ng·∫´u nhi√™n {random_page}... (ƒê√£ t√¨m ƒë∆∞·ª£c {len(book_urls)}/{num_books} s√°ch)")
                
                # URL format: /Sach/Index/0/0/0/[page_number]
                if random_page == 1:
                    page_url = f"{self.base_url}/Sach/Index"
                else:
                    page_url = f"{self.base_url}/Sach/Index/0/0/0/{random_page}"
                
                response = self.session.get(page_url)
                response.raise_for_status()
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # Find book links
                book_links = soup.find_all('a', href=re.compile(r'/Sach/Detail/\d+'))
                
                if book_links:
                    # Randomly select some books from this page
                    random.shuffle(book_links)
                    books_needed = num_books - len(book_urls)
                    books_to_take = min(len(book_links), books_needed, 3)  # Take max 3 books per page
                    
                    for i in range(books_to_take):
                        book_url = urljoin(self.base_url, book_links[i]['href'])
                        if book_url not in book_urls:
                            book_urls.append(book_url)
                
                # Add delay to be respectful to the server
                time.sleep(random.uniform(1, 2))
                
            except Exception as e:
                print(f"‚ö†Ô∏è L·ªói khi l·∫•y trang {random_page}: {str(e)}")
            
            attempts += 1
        
        print(f"‚úÖ ƒê√£ thu th·∫≠p {len(book_urls)} URL s√°ch ng·∫´u nhi√™n")
        return book_urls[:num_books]  # Ensure we don't exceed the requested number

    def scrape_multiple_books(self, book_urls):
        """Scrape multiple books from a list of URLs"""
        books_data = []
        
        print(f"üöÄ B·∫Øt ƒë·∫ßu craw {len(book_urls)} quy·ªÉn s√°ch...")
        
        for i, url in enumerate(book_urls):
            print(f"\nüìö ƒêang craw s√°ch {i+1}/{len(book_urls)}: {url}")
            
            book_info = self.extract_book_info(url)
            if book_info:
                books_data.append(book_info)
            else:
                print(f"‚ùå Kh√¥ng th·ªÉ craw ƒë∆∞·ª£c s√°ch t·ª´ {url}")
            
            # Add delay to be respectful to the server
            time.sleep(random.uniform(2, 4))
        
        return books_data

    def save_to_json(self, books_data, file_path="duytan_books.json"):
        """Save book data to JSON file"""
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(books_data, f, ensure_ascii=False, indent=2)
        print(f"‚úì ƒê√£ l∆∞u {len(books_data)} s√°ch v√†o {file_path}")
    
    def save_to_sqlite(self, books_data, db_path="duytan_books.db"):
        """Save book data to SQLite database"""
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        
        # Create table
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS books (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                title TEXT,
                author TEXT,
                publisher TEXT,
                publication_year INTEGER,
                pages TEXT,
                dimensions TEXT,
                registration_number TEXT,
                isbn TEXT,
                dewey_code TEXT,
                price TEXT,
                storage_location TEXT,
                language TEXT,
                document_type TEXT,
                availability TEXT,
                keywords TEXT,
                subject TEXT,
                department TEXT,
                summary TEXT,
                cover_image TEXT,
                url TEXT,
                scraped_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # Insert data
        for book in books_data:
            cursor.execute('''
                INSERT INTO books (
                    title, author, publisher, publication_year, pages, dimensions,
                    registration_number, isbn, dewey_code, price, storage_location,
                    language, document_type, availability, keywords, subject,
                    department, summary, cover_image, url
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                book.get('title', ''),
                book.get('author', ''),
                book.get('publisher', ''),
                book.get('publication_year', 0),
                book.get('pages', ''),
                book.get('dimensions', ''),
                book.get('registration_number', ''),
                book.get('isbn', ''),
                book.get('dewey_code', ''),
                book.get('price', ''),
                book.get('storage_location', ''),
                book.get('language', ''),
                book.get('document_type', ''),
                book.get('availability', ''),
                book.get('keywords', ''),
                book.get('subject', ''),
                book.get('department', ''),
                book.get('summary', ''),
                book.get('cover_image', ''),
                book.get('url', '')
            ))
        
        conn.commit()
        conn.close()
        print(f"‚úì ƒê√£ l∆∞u {len(books_data)} s√°ch v√†o {db_path}")

def main():
    """Main function to crawl 36 random books"""
    scraper = DuyTanLibraryScraper()
    
    print("=== B·∫Øt ƒë·∫ßu craw 36 cu·ªën s√°ch ng·∫´u nhi√™n t·ª´ th∆∞ vi·ªán Duy T√¢n ===")
    
    # Get random books
    random_book_urls = scraper.get_random_books(36)
    
    if not random_book_urls:
        print("‚ùå Kh√¥ng th·ªÉ t√¨m ƒë∆∞·ª£c s√°ch ng·∫´u nhi√™n")
        return False
    
    print(f"üìã Danh s√°ch {len(random_book_urls)} URL s√°ch ng·∫´u nhi√™n:")
    for i, url in enumerate(random_book_urls[:5]):  # Show first 5 URLs
        print(f"  {i+1}. {url}")
    if len(random_book_urls) > 5:
        print(f"  ... v√† {len(random_book_urls) - 5} URL kh√°c")
    
    # Scrape books data
    books_data = scraper.scrape_multiple_books(random_book_urls)
    
    if books_data:
        print(f"\n‚úÖ ƒê√£ craw ƒë∆∞·ª£c {len(books_data)} cu·ªën s√°ch th√†nh c√¥ng!")
        
        # L∆∞u v√†o file
        scraper.save_to_json(books_data, "random_36_books.json")
        scraper.save_to_sqlite(books_data, "random_36_books.db")
        
        # Hi·ªÉn th·ªã th·ªëng k√™
        print(f"\nüìä Th·ªëng k√™:")
        print(f"   - T·ªïng s·ªë s√°ch: {len(books_data)}")
        
        # Count by language
        languages = {}
        for book in books_data:
            lang = book.get('language', 'Kh√¥ng r√µ')
            languages[lang] = languages.get(lang, 0) + 1
        
        print("   - Ph√¢n b·ªë ng√¥n ng·ªØ:")
        for lang, count in languages.items():
            print(f"     ‚Ä¢ {lang}: {count} s√°ch")
        
        # Show sample books
        print(f"\nüìö M·∫´u s√°ch (3 cu·ªën ƒë·∫ßu):")
        for i, book in enumerate(books_data[:3]):
            print(f"   {i+1}. {book.get('title', 'Kh√¥ng c√≥ ti√™u ƒë·ªÅ')}")
            print(f"      T√°c gi·∫£: {book.get('author', 'N/A')}")
            print(f"      NƒÉm: {book.get('publication_year', 'N/A')}")
            print(f"      Ng√¥n ng·ªØ: {book.get('language', 'N/A')}")
        
        print(f"\nüéâ Ho√†n th√†nh! D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o:")
        print("   - random_36_books.json")
        print("   - random_36_books.db")
        
        return True
    else:
        print("‚ùå Kh√¥ng craw ƒë∆∞·ª£c s√°ch n√†o")
        return False

if __name__ == "__main__":
    main()
