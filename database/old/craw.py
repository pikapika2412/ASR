import requests
from bs4 import BeautifulSoup
import sqlite3
import json
import time
import re
import random
from urllib.parse import urljoin

class DuyTanLibraryScraper:
    def __init__(self):
        self.base_url = "https://elib.duytan.edu.vn"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        
    def extract_book_info(self, book_url):
        """Extract detailed information from a book page"""
        try:
            print(f"Äang trÃ­ch xuáº¥t thÃ´ng tin tá»«: {book_url}")
            response = self.session.get(book_url)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser')
            
            book_info = {}
            
            # Extract all text content and parse structured information
            page_text = soup.get_text()            # Extract book title - improved method
            # TÃ¬m text chÃ­nh xÃ¡c giá»¯a áº£nh vÃ  metadata Ä‘áº§u tiÃªn
            
            # Method 1: TÃ¬m báº±ng regex pattern sau áº£nh vÃ  trÆ°á»›c "TÃ¡c giáº£:"
            pattern = r'\.jpg\s*\n\s*(.*?)(?=TÃ¡c giáº£:)'
            title_match = re.search(pattern, page_text, re.DOTALL)
            
            if title_match:
                potential_text = title_match.group(1).strip()
                # TÃ¡ch thÃ nh dÃ²ng vÃ  tÃ¬m dÃ²ng title tháº­t
                lines = potential_text.split('\n')
                for line in lines:
                    line = line.strip()
                    # Bá» qua cÃ¡c dÃ²ng navigation vÃ  láº¥y content title
                    if (line and len(line) > 5 and len(line) < 300 and
                        not line.startswith('Trá»Ÿ vá»') and
                        not line.startswith('Hiá»ƒn thá»‹') and
                        not line.startswith('CSDL') and
                        not line.startswith('SÃ¡ch') and
                        'Trang chá»§' not in line and
                        'Marc' not in line and
                        'jpg' not in line):
                        # ÄÃ¢y cÃ³ thá»ƒ lÃ  title tháº­t
                        book_info['title'] = line
                        break
            
            # Method 2: Náº¿u váº«n chÆ°a cÃ³, thá»­ tÃ¬m báº±ng cÃ¡ch khÃ¡c
            if 'title' not in book_info:
                # TÃ¬m trong HTML structure cá»¥ thá»ƒ
                main_content = soup.find('div', class_='col-md-8')
                if main_content:
                    # Láº¥y text vÃ  tÃ¬m pháº§n giá»¯a áº£nh vÃ  metadata
                    content_text = main_content.get_text()
                    
                    # Pattern: sau "jpg" vÃ  trÆ°á»›c cÃ¡c metadata fields
                    alt_pattern = r'jpg.*?\n\s*([^\n]+?)(?=\s*(?:TÃ¡c giáº£:|NhÃ  xuáº¥t báº£n:|NÄƒm xuáº¥t báº£n:))'
                    alt_match = re.search(alt_pattern, content_text, re.DOTALL)
                    
                    if alt_match:
                        title_candidate = alt_match.group(1).strip()
                        # Clean title
                        title_candidate = re.sub(r'\s+', ' ', title_candidate)
                        
                        if (title_candidate and len(title_candidate) > 5 and
                            'Trang chá»§' not in title_candidate and
                            not title_candidate.startswith('CSDL') and
                            not title_candidate.startswith('Trá»Ÿ vá»')):
                            book_info['title'] = title_candidate
            
            # Method 3: Final fallback - scan for reasonable title
            if 'title' not in book_info:
                content_div = soup.find('div', class_='col-md-8')
                if content_div:
                    paragraphs = content_div.find_all(['p', 'div', 'span'])
                    for p in paragraphs:
                        text = p.get_text().strip()
                        if (text and len(text) > 10 and len(text) < 200 and
                            not text.startswith(('TÃ¡c giáº£:', 'NhÃ  xuáº¥t báº£n:', 'NÄƒm xuáº¥t báº£n:', 
                                              'Sá»‘ trang:', 'KÃ­ch thÆ°á»›c:', 'ISBN:', 'MÃ£ Dewey:',
                                              'Trá»Ÿ vá»', 'Hiá»ƒn thá»‹', 'CSDL')) and
                            'Trang chá»§' not in text and
                            'upload/sach_anh' not in text):
                            book_info['title'] = text
                            break
            
            # Extract book cover image
            img_element = soup.find('img', src=re.compile(r'sach_anh'))
            if img_element:
                book_info['cover_image'] = urljoin(self.base_url, img_element['src'])
            
            # Extract author - TÃ¡c giáº£
            author_match = re.search(r'TÃ¡c giáº£:\s*([^\n\r]+)', page_text)
            if author_match:
                book_info['author'] = author_match.group(1).strip()
            
            # Extract publisher - NhÃ  xuáº¥t báº£n
            publisher_match = re.search(r'NhÃ  xuáº¥t báº£n:\s*([^\n\r]+)', page_text)
            if publisher_match:
                book_info['publisher'] = publisher_match.group(1).strip()
            
            # Extract publication year - NÄƒm xuáº¥t báº£n
            year_match = re.search(r'NÄƒm xuáº¥t báº£n:\s*(\d{4})', page_text)
            if year_match:
                book_info['publication_year'] = int(year_match.group(1))
            
            # Extract number of pages - Sá»‘ trang
            pages_match = re.search(r'Sá»‘ trang:\s*([^\n\r]+)', page_text)
            if pages_match:
                book_info['pages'] = pages_match.group(1).strip()
            
            # Extract dimensions - KÃ­ch thÆ°á»›c
            dimensions_match = re.search(r'KÃ­ch thÆ°á»›c:\s*([^\n\r]+)', page_text)
            if dimensions_match:
                book_info['dimensions'] = dimensions_match.group(1).strip()
            
            # Extract registration number - Sá»‘ Ä‘Äƒng kÃ½ cÃ¡ biá»‡t
            reg_number_match = re.search(r'Sá»‘ Ä‘Äƒng kÃ½ cÃ¡ biá»‡t:\s*([^\n\r]+)', page_text)
            if reg_number_match:
                book_info['registration_number'] = reg_number_match.group(1).strip()
            
            # Extract ISBN
            isbn_match = re.search(r'ISBN:\s*([^\n\r]+)', page_text)
            if isbn_match:
                book_info['isbn'] = isbn_match.group(1).strip()
            
            # Extract Dewey code - MÃ£ Dewey
            dewey_match = re.search(r'MÃ£ Dewey:\s*([^\n\r]+)', page_text)
            if dewey_match:
                book_info['dewey_code'] = dewey_match.group(1).strip()
            
            # Extract price - ÄÆ¡n giÃ¡
            price_match = re.search(r'ÄÆ¡n giÃ¡:\s*([^\n\r]+)', page_text)
            if price_match:
                book_info['price'] = price_match.group(1).strip()
            
            # Extract storage location - Vá»‹ trÃ­ lÆ°u trá»¯
            location_match = re.search(r'Vá»‹ trÃ­ lÆ°u trá»¯:\s*([^\n\r]+)', page_text)
            if location_match:
                book_info['storage_location'] = location_match.group(1).strip()
            
            # Extract language - NgÃ´n ngá»¯
            language_match = re.search(r'NgÃ´n ngá»¯:\s*([^\n\r]+)', page_text)
            if language_match:
                book_info['language'] = language_match.group(1).strip()
            
            # Extract document type - Loáº¡i tÃ i liá»‡u
            doc_type_match = re.search(r'Loáº¡i tÃ i liá»‡u:\s*([^\n\r]+)', page_text)
            if doc_type_match:
                book_info['document_type'] = doc_type_match.group(1).strip()
            
            # Extract availability - Äang rá»—i/ Tá»•ng sÃ¡ch
            availability_match = re.search(r'Äang rá»—i/ Tá»•ng sÃ¡ch:\s*([^\n\r]+)', page_text)
            if availability_match:
                book_info['availability'] = availability_match.group(1).strip()
            
            # Extract keywords - Tá»« khÃ³a
            keywords_match = re.search(r'Tá»« khÃ³a:\s*([^\n\r]+)', page_text)
            if keywords_match:
                book_info['keywords'] = keywords_match.group(1).strip()
            
            # Extract subject - Chá»§ Ä‘á»
            subject_match = re.search(r'Chá»§ Ä‘á»:\s*([^\n\r]+)', page_text)
            if subject_match:
                book_info['subject'] = subject_match.group(1).strip()
            
            # Extract department - ChuyÃªn ngÃ nh
            department_match = re.search(r'ChuyÃªn ngÃ nh:\s*([^\n\r]+)', page_text)
            if department_match:
                book_info['department'] = department_match.group(1).strip()
              # Extract summary - TÃ³m táº¯t
            summary_match = re.search(r'TÃ³m táº¯t:\s*(.*?)(?=\n\s*\n|\n\s*[A-Z]|$)', page_text, re.DOTALL)
            if summary_match:
                summary = summary_match.group(1).strip()
                summary = re.sub(r'\s+', ' ', summary)
                summary = summary.split('Báº¡n pháº£i Ä‘Äƒng nháº­p')[0].strip()
                if summary:
                    book_info['summary'] = summary
            
            # Add URL for reference
            book_info['url'] = book_url
            
            print(f"âœ“ ÄÃ£ trÃ­ch xuáº¥t thÃ nh cÃ´ng: {book_info.get('title', 'KhÃ´ng cÃ³ tiÃªu Ä‘á»')}")
            return book_info
            
        except Exception as e:
            print(f"âœ— Lá»—i khi trÃ­ch xuáº¥t thÃ´ng tin tá»« {book_url}: {str(e)}")
            return {}

    def save_to_json(self, books_data, file_path="duytan_books.json"):
        """Save book data to JSON file"""
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(books_data, f, ensure_ascii=False, indent=2)
        print(f"âœ“ ÄÃ£ lÆ°u {len(books_data)} sÃ¡ch vÃ o {file_path}")
    
    def save_to_sqlite(self, books_data, db_path="duytan_books.db"):
        """Save book data to SQLite database"""
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        
        # Create table
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS books (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                title TEXT,
                author TEXT,
                publisher TEXT,
                publication_year INTEGER,
                pages TEXT,
                dimensions TEXT,
                registration_number TEXT,
                isbn TEXT,
                dewey_code TEXT,
                price TEXT,
                storage_location TEXT,
                language TEXT,
                document_type TEXT,
                availability TEXT,
                keywords TEXT,
                subject TEXT,
                department TEXT,
                summary TEXT,
                cover_image TEXT,
                url TEXT,
                scraped_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # Insert data
        for book in books_data:
            cursor.execute('''
                INSERT INTO books (
                    title, author, publisher, publication_year, pages, dimensions,
                    registration_number, isbn, dewey_code, price, storage_location,
                    language, document_type, availability, keywords, subject,
                    department, summary, cover_image, url
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                book.get('title', ''),
                book.get('author', ''),
                book.get('publisher', ''),
                book.get('publication_year', 0),
                book.get('pages', ''),
                book.get('dimensions', ''),
                book.get('registration_number', ''),
                book.get('isbn', ''),
                book.get('dewey_code', ''),
                book.get('price', ''),
                book.get('storage_location', ''),
                book.get('language', ''),
                book.get('document_type', ''),
                book.get('availability', ''),
                book.get('keywords', ''),
                book.get('subject', ''),
                book.get('department', ''),
                book.get('summary', ''),
                book.get('cover_image', ''),
                book.get('url', '')
            ))
        
        conn.commit()
        conn.close()
        print(f"âœ“ ÄÃ£ lÆ°u {len(books_data)} sÃ¡ch vÃ o {db_path}")

    def get_random_books(self, num_books=36):
        """Get random book URLs from random pages in the library"""
        book_urls = []
        attempts = 0
        max_attempts = num_books * 3  # Try 3 times more than needed
        
        print(f"ğŸ” Báº¯t Ä‘áº§u thu tháº­p {num_books} quyá»ƒn sÃ¡ch ngáº«u nhiÃªn...")
        
        while len(book_urls) < num_books and attempts < max_attempts:
            # Generate random page number (limit to first 200 pages for efficiency)
            random_page = random.randint(1, 200)
            
            try:
                print(f"ğŸ“– Äang láº¥y trang ngáº«u nhiÃªn {random_page}... (ÄÃ£ tÃ¬m Ä‘Æ°á»£c {len(book_urls)}/{num_books} sÃ¡ch)")
                
                # URL format: /Sach/Index/0/0/0/[page_number]
                if random_page == 1:
                    page_url = f"{self.base_url}/Sach/Index"
                else:
                    page_url = f"{self.base_url}/Sach/Index/0/0/0/{random_page}"
                
                response = self.session.get(page_url)
                response.raise_for_status()
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # Find book links
                book_links = soup.find_all('a', href=re.compile(r'/Sach/Detail/\d+'))
                
                if book_links:
                    # Randomly select some books from this page
                    random.shuffle(book_links)
                    books_needed = num_books - len(book_urls)
                    books_to_take = min(len(book_links), books_needed, 3)  # Take max 3 books per page
                    
                    for i in range(books_to_take):
                        book_url = urljoin(self.base_url, book_links[i]['href'])
                        if book_url not in book_urls:
                            book_urls.append(book_url)
                
                # Add delay to be respectful to the server
                time.sleep(random.uniform(1, 2))
                
            except Exception as e:
                print(f"âš ï¸ Lá»—i khi láº¥y trang {random_page}: {str(e)}")
            
            attempts += 1
        
        print(f"âœ… ÄÃ£ thu tháº­p {len(book_urls)} URL sÃ¡ch ngáº«u nhiÃªn")
        return book_urls[:num_books]  # Ensure we don't exceed the requested number

    def scrape_multiple_books(self, book_urls):
        """Scrape multiple books from a list of URLs"""
        books_data = []
        
        print(f"ğŸš€ Báº¯t Ä‘áº§u craw {len(book_urls)} quyá»ƒn sÃ¡ch...")
        
        for i, url in enumerate(book_urls):
            print(f"\nğŸ“š Äang craw sÃ¡ch {i+1}/{len(book_urls)}: {url}")
            
            book_info = self.extract_book_info(url)
            if book_info:
                books_data.append(book_info)
            else:
                print(f"âŒ KhÃ´ng thá»ƒ craw Ä‘Æ°á»£c sÃ¡ch tá»« {url}")
            
            # Add delay to be respectful to the server
            time.sleep(random.uniform(2, 4))
        
        return books_data

def test_single_book():
    """Test scraping a single book"""
    scraper = DuyTanLibraryScraper()
    
    # Test vá»›i sÃ¡ch Elon Musk mÃ  báº¡n Ä‘Ã£ Ä‘á» cáº­p
    test_url = "https://elib.duytan.edu.vn/Sach/Detail/92799"
    
    print("=== Test craw má»™t cuá»‘n sÃ¡ch ===")
    print(f"URL test: {test_url}")
    
    book_info = scraper.extract_book_info(test_url)
    
    if book_info:
        print("\n=== ThÃ´ng tin sÃ¡ch Ä‘Ã£ craw ===")
        for key, value in book_info.items():
            print(f"{key}: {value}")
        
        # LÆ°u vÃ o file
        scraper.save_to_json([book_info], "single_book.json")
        scraper.save_to_sqlite([book_info], "single_book.db")
        
        return True
    else:
        print("KhÃ´ng thá»ƒ craw Ä‘Æ°á»£c thÃ´ng tin sÃ¡ch")
        return False

def test_scrape_multiple_books():
    """Test scraping multiple books"""
    scraper = DuyTanLibraryScraper()
    
    # Get random books
    random_book_urls = scraper.get_random_books(5)
    
    print("\n=== Test craw nhiá»u cuá»‘n sÃ¡ch ngáº«u nhiÃªn ===")
    for url in random_book_urls:
        print(f"- {url}")
    
    # Scrape books data
    books_data = scraper.scrape_multiple_books(random_book_urls)
    
    print(f"\nâœ… ÄÃ£ craw Ä‘Æ°á»£c {len(books_data)} cuá»‘n sÃ¡ch")
    
    # LÆ°u vÃ o file
    scraper.save_to_json(books_data, "multiple_books.json")
    scraper.save_to_sqlite(books_data, "multiple_books.db")

def test_scrape_random_books():
    """Test scraping 36 random books"""
    scraper = DuyTanLibraryScraper()
    
    print("\n=== Test craw 36 cuá»‘n sÃ¡ch ngáº«u nhiÃªn ===")
    
    # Get random books
    random_book_urls = scraper.get_random_books(36)
    
    if not random_book_urls:
        print("âŒ KhÃ´ng thá»ƒ tÃ¬m Ä‘Æ°á»£c sÃ¡ch ngáº«u nhiÃªn")
        return False
    
    print(f"ğŸ“‹ Danh sÃ¡ch {len(random_book_urls)} URL sÃ¡ch ngáº«u nhiÃªn:")
    for i, url in enumerate(random_book_urls[:5]):  # Show first 5 URLs
        print(f"  {i+1}. {url}")
    if len(random_book_urls) > 5:
        print(f"  ... vÃ  {len(random_book_urls) - 5} URL khÃ¡c")
    
    # Scrape books data
    books_data = scraper.scrape_multiple_books(random_book_urls)
    
    if books_data:
        print(f"\nâœ… ÄÃ£ craw Ä‘Æ°á»£c {len(books_data)} cuá»‘n sÃ¡ch thÃ nh cÃ´ng!")
        
        # LÆ°u vÃ o file
        scraper.save_to_json(books_data, "random_36_books.json")
        scraper.save_to_sqlite(books_data, "random_36_books.db")
        
        # Hiá»ƒn thá»‹ thá»‘ng kÃª
        print(f"\nğŸ“Š Thá»‘ng kÃª:")
        print(f"   - Tá»•ng sá»‘ sÃ¡ch: {len(books_data)}")
        
        # Count by language
        languages = {}
        for book in books_data:
            lang = book.get('language', 'KhÃ´ng rÃµ')
            languages[lang] = languages.get(lang, 0) + 1
        
        print("   - PhÃ¢n bá»‘ ngÃ´n ngá»¯:")
        for lang, count in languages.items():
            print(f"     â€¢ {lang}: {count} sÃ¡ch")
        
        # Show sample books
        print(f"\nğŸ“š Máº«u sÃ¡ch (3 cuá»‘n Ä‘áº§u):")
        for i, book in enumerate(books_data[:3]):
            print(f"   {i+1}. {book.get('title', 'KhÃ´ng cÃ³ tiÃªu Ä‘á»')}")
            print(f"      TÃ¡c giáº£: {book.get('author', 'N/A')}")
            print(f"      NÄƒm: {book.get('publication_year', 'N/A')}")
            print(f"      NgÃ´n ngá»¯: {book.get('language', 'N/A')}")
        
        return True
    else:
        print("âŒ KhÃ´ng craw Ä‘Æ°á»£c sÃ¡ch nÃ o")
        return False

if __name__ == "__main__":
    print("ğŸš€ DuyTan Library Scraper")
    print("1. Test craw má»™t cuá»‘n sÃ¡ch")
    print("2. Craw 36 cuá»‘n sÃ¡ch ngáº«u nhiÃªn")
    
    choice = input("Chá»n cháº¿ Ä‘á»™ (1 hoáº·c 2): ").strip()
    
    if choice == "1":
        print("\n=== Cháº¡y test craw má»™t cuá»‘n sÃ¡ch ===")
        success = test_single_book()
        
        if success:
            print("\nâœ… Test thÃ nh cÃ´ng! Code Ä‘Ã£ hoáº¡t Ä‘á»™ng tá»‘t.")
            print("Báº¡n cÃ³ thá»ƒ kiá»ƒm tra file 'single_book.json' vÃ  'single_book.db'")
        else:
            print("\nâŒ Test tháº¥t báº¡i. Cáº§n kiá»ƒm tra láº¡i code.")
            
    elif choice == "2":
        print("\n=== Báº¯t Ä‘áº§u craw 36 cuá»‘n sÃ¡ch ngáº«u nhiÃªn ===")
        success = test_scrape_random_books()
        
        if success:
            print("\nğŸ‰ HoÃ n thÃ nh! Dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c lÆ°u vÃ o:")
            print("   - random_36_books.json")
            print("   - random_36_books.db")
        else:
            print("\nâŒ CÃ³ lá»—i xáº£y ra trong quÃ¡ trÃ¬nh craw.")
    else:
        print("âŒ Lá»±a chá»n khÃ´ng há»£p lá»‡. Vui lÃ²ng chá»n 1 hoáº·c 2.")